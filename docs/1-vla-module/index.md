# Vision-Language-Action (VLA) Module

Welcome to the Vision-Language-Action (VLA) module of the Physical AI & Humanoid Robotics Book. This module teaches students how LLMs, vision, and robot actions integrate into a single pipeline.

## Overview

This module focuses on:
- Voice-to-Action using Whisper
- LLM-based cognitive planning
- End-to-end autonomous humanoid workflow (capstone)

## Chapters

1. [Voice-to-Action Basics (Whisper → Commands)](./chapter-1-voice-to-action.md)
2. [Cognitive Planning with LLMs (Natural Language → ROS2 Actions)](./chapter-2-cognitive-planning.md)
3. [Capstone: Autonomous Humanoid (Perception + Planning + Action)](./chapter-3-capstone.md)

## Learning Objectives

After completing this module, students will be able to:
- Understand how Whisper, LLMs, and ROS2 coordinate in the VLA pipeline
- Implement a complete voice-to-action pipeline
- Design cognitive planning workflows with natural language processing
- Create an end-to-end autonomous humanoid system